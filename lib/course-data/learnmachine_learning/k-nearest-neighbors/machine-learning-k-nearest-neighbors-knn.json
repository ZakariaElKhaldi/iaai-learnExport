{
  "id": "def6efaa-d869-472a-b188-df44b95fa34a",
  "title": "Machine Learning - K-nearest neighbors (KNN)",
  "slug": "machine-learning-k-nearest-neighbors-knn",
  "metadata": {
    "description": "Learn about Machine Learning - K-nearest neighbors (KNN) with clear explanations and practical examples.",
    "keywords": [
      "c",
      "data",
      "class",
      "prediction",
      "point",
      "classes",
      "result",
      "points",
      "show",
      "example",
      "predict"
    ],
    "difficulty": "beginner",
    "prerequisites": [
      "C Basics"
    ],
    "estimated_time": 5,
    "category": "LearnMachine Learning",
    "subcategory": "K-nearest neighbors"
  },
  "content_sections": [
    {
      "type": "introduction",
      "title": "Introduction",
      "content": "Machine Learning - K-nearest neighbors (KNN)",
      "order": 1,
      "code": null,
      "language": null,
      "explanation": null
    },
    {
      "type": "concept",
      "title": "Details",
      "content": "On this page, .com collaborates with\nNYC Data Science Academy\n, to deliver digital training content to our students.\nKNN\nKNN is a simple, supervised machine learning (ML) algorithm that can be used for classification or regression tasks - and is also frequently used in missing value imputation. It is based on the idea that the observations closest to a given data point are the most \"similar\" observations in a data set, and we can therefore classify unforeseen points based on the values of the closest existing points. By choosing\nK\n, the user can select the number of nearby observations to use in the algorithm.\nHere, we will show you how to implement the KNN algorithm for classification, and show how different values of\nK\naffect the results.\nHow does it work?\nK\nis the number of nearest neighbors to use. \nFor classification, a majority vote is used to determined which class a new observation should fall into. \nLarger values of\nK\nare often more robust to outliers and produce more stable decision boundaries than\nvery small values (\nK=3\nwould be better than\nK=1\n, which might produce undesirable results.\nExample\nStart by visualizing some data points:\nimport matplotlib.pyplot as plt\nx = [4, 5, 10, 4, 3, 11, 14 , 8, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\nclasses = [0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\nplt.scatter(x, y, c=classes)\nplt.show()\nResult\nRun example \u00bb\nADVERTISEMENT\nNow we fit the KNN algorithm with K=1:\nfrom sklearn.neighbors import KNeighborsClassifier\ndata = list(zip(x, y))\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(data, classes)\nAnd use it to classify a new data point:\nExample\nnew_x = 8\nnew_y = 21\nnew_point = [(new_x, new_y)]\nprediction = knn.predict(new_point)\nplt.scatter(x + [new_x], y + [new_y], c=classes + [prediction[0]])\nplt.text(x=new_x-1.7, y=new_y-0.7, s=f\"new point, class: {prediction[0]}\")\nplt.show()\nResult\nRun example \u00bb\nNow we do the same thing, but with a higher K value which changes the prediction:\nExample\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(data, classes)\nprediction = knn.predict(new_point)\nplt.scatter(x + [new_x], y + [new_y], c=classes + [prediction[0]])\nplt.text(x=new_x-1.7, y=new_y-0.7, s=f\"new point, class: {prediction[0]}\")\nplt.show()\nResult\nRun example \u00bb\nExample Explained\nImport the modules you need.\nYou can learn about the Matplotlib module in our\n\"Matplotlib Tutorial\n.\nscikit-learn is a popular library for machine learning in Python.\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nCreate arrays that resemble variables in a dataset. \nWe have two input features (\nx\nand\ny\n) and then a target class (\nclass\n). The input features that are pre-labeled with our target class will be used to predict the class of new data. Note that while we only use two input features here, this method will work with any number of variables:\nx = [4, 5, 10, 4, 3, 11, 14 , 8, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\nclasses = [0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\nTurn the input features into a set of points:\ndata = list(zip(x, y))\nprint(data)\nResult:\n[(4, 21), (5, 19), (10, 24), (4, 17), (3, 16), (11, 25), (14, 24), (8, 22), (10, 21), (12, 21)]\nUsing the input features and target class, we fit a KNN model on the model using 1 nearest neighbor:\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(data, classes)\nThen, we can use the same KNN object to predict the class of new, \nunforeseen data points. First we create new x and y features, and then call\nknn.predict()\non the new data point to get a class of 0 or 1:\nnew_x = 8\nnew_y = 21\nnew_point = [(new_x, new_y)]\nprediction = knn.predict(new_point)\nprint(prediction)\nResult:\n[0]\nWhen we plot all the data along with the new point and class, we can see it's been labeled blue with the\n1\nclass. The text annotation is just to highlight the location of the new point:\nplt.scatter(x + [new_x], y + [new_y], c=classes + [prediction[0]])\nplt.text(x=new_x-1.7, y=new_y-0.7, s=f\"new point, class: {prediction[0]}\")\nplt.show()\nResult:\nHowever, when we changes the number of neighbors to 5, the number of points used to classify our new point changes. As a result, so does the classification of the new point:\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(data, classes)\nprediction = knn.predict(new_point)\nprint(prediction)\nResult:\n[1]\nWhen we plot the class of the new point along with the older points, we note that the color has changed based on the associated class label:\nplt.scatter(x + [new_x], y + [new_y], c=classes + [prediction[0]])\nplt.text(x=new_x-1.7, y=new_y-0.7, s=f\"new point, class: {prediction[0]}\")\nplt.show()\nResult:\n\n\u2605\n+1",
      "order": 2,
      "code": null,
      "language": null,
      "explanation": null
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 3,
      "code": "import matplotlib.pyplot as plt\nx = [4, 5, 10, 4, 3, 11, 14 , 8, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\nclasses = [0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\nplt.scatter(x, y, c=classes)\nplt.show()",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 4,
      "code": "new_x = 8\nnew_y = 21\nnew_point = [(new_x, new_y)]\nprediction = knn.predict(new_point)\nplt.scatter(x + [new_x], y + [new_y], c=classes + [prediction[0]])\nplt.text(x=new_x-1.7, y=new_y-0.7, s=f\"new point, class: {prediction[0]}\")\nplt.show()",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 5,
      "code": "knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(data, classes)\nprediction = knn.predict(new_point)\nplt.scatter(x + [new_x], y + [new_y], c=classes + [prediction[0]])\nplt.text(x=new_x-1.7, y=new_y-0.7, s=f\"new point, class: {prediction[0]}\")\nplt.show()",
      "language": "python",
      "explanation": "Example of example"
    }
  ],
  "practice_exercises": [
    {
      "title": "Complete the Code 1",
      "description": "Fill in the missing line to make this code work.",
      "difficulty": "medium",
      "starter_code": "import matplotlib.pyplot as plt\nx = [4, 5, 10, 4, 3, 11, 14 , 8, 10, 12]\n# TODO: Complete this line\nclasses = [0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\nplt.scatter(x, y, c=classes)\nplt.show()",
      "solution": "import matplotlib.pyplot as plt\nx = [4, 5, 10, 4, 3, 11, 14 , 8, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\nclasses = [0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\nplt.scatter(x, y, c=classes)\nplt.show()"
    },
    {
      "title": "Complete the Code 2",
      "description": "Fill in the missing line to make this code work.",
      "difficulty": "medium",
      "starter_code": "new_x = 8\nnew_y = 21\nnew_point = [(new_x, new_y)]\nprediction = knn.predict(new_point)\n# TODO: Complete this line\nplt.text(x=new_x-1.7, y=new_y-0.7, s=f\"new point, class: {prediction[0]}\")\nplt.show()",
      "solution": "new_x = 8\nnew_y = 21\nnew_point = [(new_x, new_y)]\nprediction = knn.predict(new_point)\nplt.scatter(x + [new_x], y + [new_y], c=classes + [prediction[0]])\nplt.text(x=new_x-1.7, y=new_y-0.7, s=f\"new point, class: {prediction[0]}\")\nplt.show()"
    }
  ],
  "related_topics": [
    {
      "id": "87dca6f8-e26d-45b4-b15a-4db07c867ccf",
      "title": "C Best Practices",
      "relationship": "related_topic"
    },
    {
      "id": "6066b31f-9242-463c-ae05-4a8b15b41f3e",
      "title": "C Common Pitfalls and How to Avoid Them",
      "relationship": "suggested_reading"
    },
    {
      "id": "f3f6c71d-851f-4707-b3dc-c955cdbf7ed7",
      "title": "C Fundamentals",
      "relationship": "prerequisite"
    }
  ],
  "quiz": [
    {
      "question": "What is KNN\nKNN?",
      "options": [
        "None of the above.",
        "a simple",
        "None of the above.",
        "the most \"similar\" observations in a data set"
      ],
      "correct_answer": 1,
      "explanation": "The correct definition of KNN\nKNN is 'a simple'."
    },
    {
      "question": "What is It is based on the idea that the observations closest to a given data point?",
      "options": [
        "a simple",
        "the most \"similar\" observations in a data set",
        "None of the above.",
        "None of the above."
      ],
      "correct_answer": 1,
      "explanation": "The correct definition of It is based on the idea that the observations closest to a given data point is 'the most \"similar\" observations in a data set'."
    }
  ],
  "summary": "This tutorial covers Machine Learning - K-nearest neighbors (KNN) concepts and techniques. You'll learn how to use Machine Learning - K-nearest neighbors (KNN) effectively, including key principles, common patterns, and practical examples. By the end of this tutorial, you'll have a solid understanding of Machine Learning - K-nearest neighbors (KNN) and how to apply it in your projects."
}