{
  "id": "d1b41c04-c927-49b5-aad0-bccdc5180f7d",
  "title": "Machine Learning - Hierarchical Clustering",
  "slug": "machine-learning-hierarchical-clustering",
  "metadata": {
    "description": "Learn about Machine Learning - Hierarchical Clustering with clear explanations and practical examples.",
    "keywords": [
      "c",
      "data",
      "linkage",
      "import",
      "clustering",
      "points",
      "clusters",
      "method",
      "dendrogram",
      "cluster",
      "euclidean"
    ],
    "difficulty": "beginner",
    "prerequisites": [
      "C Basics"
    ],
    "estimated_time": 5,
    "category": "LearnMachine Learning",
    "subcategory": "Hierarchical Clustering"
  },
  "content_sections": [
    {
      "type": "introduction",
      "title": "Introduction",
      "content": "Machine Learning - Hierarchical Clustering",
      "order": 1,
      "code": null,
      "language": null,
      "explanation": null
    },
    {
      "type": "concept",
      "title": "Details",
      "content": "On this page, .com collaborates with\nNYC Data Science Academy\n, to deliver digital training content to our students.\nHierarchical Clustering\nHierarchical clustering is an unsupervised learning method for clustering data points. The algorithm builds clusters by measuring the dissimilarities between data. Unsupervised learning means that a model does not have to be trained, and we do not need a \"target\" variable. This method can be used on any data to visualize and interpret the relationship between individual data points.\nHere we will use hierarchical clustering to group data points and visualize the clusters using both a dendrogram and scatter plot.\nHow does it work?\nWe will use Agglomerative Clustering, a type of hierarchical clustering that follows a bottom up approach. We begin by treating each data point as its own cluster. Then, we join clusters together that have the shortest distance between them to create larger clusters. This step is repeated until one large cluster is formed containing all of the data points.\nHierarchical clustering requires us to decide on both a distance and linkage method. We will use euclidean distance and the Ward linkage method, which attempts to minimize the variance between clusters.\nExample\nStart by visualizing some data points:\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = [4, 5, 10, 4, \n  3, 11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\nplt.scatter(x, y)\nplt.show()\nResult\nRun example \u00bb\nADVERTISEMENT\nNow we compute the ward linkage using euclidean distance, and visualize it using a dendrogram:\nExample\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom \n  scipy.cluster.hierarchy import dendrogram, linkage\nx = [4, 5, 10, 4, 3, \n  11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\ndata = list(zip(x, y))\nlinkage_data = linkage(data, method='ward', \n  metric='euclidean')\ndendrogram(linkage_data)\nplt.show()\nResult\nRun example \u00bb\nHere, we do the same thing with Python's scikit-learn library. Then, visualize on a 2-dimensional plot:\nExample\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster \n  import AgglomerativeClustering\nx = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\ndata = list(zip(x, y))\nhierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', \n  linkage='ward')\nlabels = hierarchical_cluster.fit_predict(data)\nplt.scatter(x, y, c=labels)\nplt.show()\nResult\nRun example \u00bb\nExample Explained\nImport the modules you need.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering\nYou can learn about the Matplotlib module in our\n\"Matplotlib Tutorial\n.\nYou can learn about the SciPy module in our\nSciPy Tutorial\n.\nNumPy is a library for working with arrays and matricies in Python,\nyou can learn about the NumPy module in our\nNumPy Tutorial\n.\nscikit-learn is a popular library for machine learning.\nCreate arrays that resemble two variables in a dataset. Note that while we only \nuse two variables here, this method will work with any number of variables:\nx = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\nTurn the data into a set of points:\ndata = list(zip(x, y))\nprint(data)\nResult:\n[(4, 21), (5, 19), (10, 24), (4, 17), (3, 16), (11, 25), (14, 24), (6, 22), (10, 21), (12, 21)]\nCompute the linkage between all of the different points. Here we use a simple euclidean distance measure and Ward's linkage, which seeks to minimize the variance between clusters.\nlinkage_data = linkage(data, method='ward', metric='euclidean')\nFinally, plot the results in a dendrogram. This plot will show us the hierarchy of clusters from the bottom (individual points) to the top (a single cluster consisting of all data points).\nplt.show()\nlets us visualize the dendrogram instead of just the raw linkage data.\ndendrogram(linkage_data)\nplt.show()\nResult:\nThe scikit-learn library allows us to use hierarchichal clustering in a different manner. First, we initialize the\nAgglomerativeClustering\nclass with 2 clusters, using the same euclidean distance and Ward linkage.\nhierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\nThe\n.fit_predict\nmethod can be called on our data to compute the clusters using the defined parameters across our chosen number of clusters.\nlabels = hierarchical_cluster.fit_predict(data)\nprint(labels)\nResult:\n[0 0 1 0 0 1 1 0 1 1]\nFinally, if we plot the same data and color the points using the labels assigned to each index by the hierarchical clustering method, we can see the cluster each point was assigned to:\nplt.scatter(x, y, c=labels)\nplt.show()\nResult:\n\n\u2605\n+1",
      "order": 2,
      "code": null,
      "language": null,
      "explanation": null
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 3,
      "code": "import numpy as np\nimport matplotlib.pyplot as plt\nx = [4, 5, 10, 4, \n  3, 11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\nplt.scatter(x, y)\nplt.show()",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 4,
      "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom \n  scipy.cluster.hierarchy import dendrogram, linkage\nx = [4, 5, 10, 4, 3, \n  11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\ndata = list(zip(x, y))\nlinkage_data = linkage(data, method='ward', \n  metric='euclidean')\ndendrogram(linkage_data)\nplt.show()",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 5,
      "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster \n  import AgglomerativeClustering\nx = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\ndata = list(zip(x, y))\nhierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', \n  linkage='ward')\nlabels = hierarchical_cluster.fit_predict(data)\nplt.scatter(x, y, c=labels)\nplt.show()",
      "language": "python",
      "explanation": "Example of example"
    }
  ],
  "practice_exercises": [
    {
      "title": "Complete the Code 1",
      "description": "Fill in the missing line to make this code work.",
      "difficulty": "medium",
      "starter_code": "import numpy as np\nimport matplotlib.pyplot as plt\nx = [4, 5, 10, 4, \n  3, 11, 14 , 6, 10, 12]\n# TODO: Complete this line\nplt.scatter(x, y)\nplt.show()",
      "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nx = [4, 5, 10, 4, \n  3, 11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\nplt.scatter(x, y)\nplt.show()"
    },
    {
      "title": "Complete the Code 2",
      "description": "Fill in the missing line to make this code work.",
      "difficulty": "medium",
      "starter_code": "import numpy as np\nimport matplotlib.pyplot as plt\n# TODO: Complete this line\n  scipy.cluster.hierarchy import dendrogram, linkage\nx = [4, 5, 10, 4, 3, \n  11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\ndata = list(zip(x, y))\nlinkage_data = linkage(data, method='ward', \n  metric='euclidean')\ndendrogram(linkage_data)\nplt.show()",
      "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom \n  scipy.cluster.hierarchy import dendrogram, linkage\nx = [4, 5, 10, 4, 3, \n  11, 14 , 6, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\ndata = list(zip(x, y))\nlinkage_data = linkage(data, method='ward', \n  metric='euclidean')\ndendrogram(linkage_data)\nplt.show()"
    }
  ],
  "related_topics": [
    {
      "id": "07685a5b-7984-4af7-ad06-6f7c7cc6e46a",
      "title": "C Best Practices",
      "relationship": "related_topic"
    },
    {
      "id": "60288d23-ffa7-47c9-b6a9-1d1ffe2aa211",
      "title": "C Common Pitfalls and How to Avoid Them",
      "relationship": "suggested_reading"
    },
    {
      "id": "d4e9c9e9-7964-4cc7-adbf-452eb23e953c",
      "title": "C Fundamentals",
      "relationship": "prerequisite"
    }
  ],
  "quiz": [
    {
      "question": "What is Hierarchical Clustering\nHierarchical clustering?",
      "options": [
        "None of the above.",
        "an unsupervised learning method for clustering data points",
        "that a model does not have to be trained",
        "None of the above."
      ],
      "correct_answer": 1,
      "explanation": "The correct definition of Hierarchical Clustering\nHierarchical clustering is 'an unsupervised learning method for clustering data points'."
    },
    {
      "question": "What is Unsupervised learning?",
      "options": [
        "that a model does not have to be trained",
        "None of the above.",
        "an unsupervised learning method for clustering data points",
        "None of the above."
      ],
      "correct_answer": 0,
      "explanation": "The correct definition of Unsupervised learning is 'that a model does not have to be trained'."
    }
  ],
  "summary": "This tutorial covers Machine Learning - Hierarchical Clustering concepts and techniques. You'll learn how to use Machine Learning - Hierarchical Clustering effectively, including key principles, common patterns, and practical examples. By the end of this tutorial, you'll have a solid understanding of Machine Learning - Hierarchical Clustering and how to apply it in your projects."
}