{
  "id": "4353ab9e-6f22-4bc2-ab3a-f68012b185e7",
  "title": "Machine Learning - Logistic Regression",
  "slug": "machine-learning-logistic-regression",
  "metadata": {
    "description": "Learn about Machine Learning - Logistic Regression with clear explanations and practical examples.",
    "keywords": [
      "c",
      "logr",
      "odds",
      "numpy",
      "tumor",
      "probability",
      "regression",
      "example",
      "cancerous",
      "logistic",
      "array"
    ],
    "difficulty": "beginner",
    "prerequisites": [
      "C Basics"
    ],
    "estimated_time": 5,
    "category": "LearnMachine Learning",
    "subcategory": "Logistic Regression"
  },
  "content_sections": [
    {
      "type": "introduction",
      "title": "Introduction",
      "content": "Machine Learning - Logistic Regression",
      "order": 1,
      "code": null,
      "language": null,
      "explanation": null
    },
    {
      "type": "concept",
      "title": "Details",
      "content": "On this page, .com collaborates with\nNYC Data Science Academy\n, to deliver digital training content to our students.\nLogistic Regression\nLogistic regression aims to solve classification problems. It does this by predicting categorical outcomes, unlike linear regression that predicts a continuous outcome.\nIn the simplest case there are two outcomes, which is called binomial, an example of which is predicting if a tumor is malignant or benign. Other cases have more than two outcomes to classify, in this case it is called multinomial. A common example for multinomial logistic regression would be predicting the class of an iris flower between 3 different species.\nHere we will be using basic logistic regression to predict a binomial variable. This means it has only two possible outcomes.\nHow does it work?\nIn Python we have modules that will do the work for us. Start by importing the NumPy module.\nimport numpy\nStore the independent variables in X.\nStore the dependent variable in y.\nBelow is a sample dataset:\n#X represents the size of a tumor in centimeters.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\n#Note: X has to be reshaped into a column from a row for the LogisticRegression() function to work.\n#y represents whether or not the tumor is cancerous (0 for \"No\", 1 for \"Yes\").\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\nWe will use a method from the sklearn module, so we will have to import that module as well:\nfrom sklearn import linear_model\nFrom the sklearn module we will use the LogisticRegression() method to create a logistic regression object.\nThis object has a method called\nfit()\nthat takes the independent and dependent values as parameters and fills the regression object with data that describes the relationship:\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\nNow we have a logistic regression object that is ready to whether a tumor is cancerous based on the tumor size:\n#predict if tumor is cancerous where the size is 3.46mm:\npredicted = logr.predict(numpy.array([3.46]).reshape(-1,1))\nExample\nSee the whole example in action:\nimport numpy\nfrom sklearn import linear_model\n#Reshaped for Logistic function.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\n#predict if tumor is cancerous where the size is 3.46mm:\npredicted = logr.predict(numpy.array([3.46]).reshape(-1,1))\nprint(predicted)\nResult\n[0]\nRun example \u00bb\nWe have predicted that a tumor with a size of 3.46mm will not be cancerous.\nADVERTISEMENT\nCoefficient\nIn logistic regression the coefficient is the expected change in log-odds of having the outcome per unit change in X.\nThis does not have the most intuitive understanding so let's use it to create something that makes more sense, odds.\nExample\nSee the whole example in action:\nimport numpy\nfrom sklearn import linear_model\n#Reshaped for Logistic function.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\nlog_odds = logr.coef_\nodds = numpy.exp(log_odds)\nprint(odds)\nResult\n[4.03541657]\nRun example \u00bb\nThis tells us that as the size of a tumor increases by 1mm the odds of it being a \ncancerous tumor increases by 4x.\nProbability\nThe coefficient and intercept values can be used to find the probability that each tumor is cancerous.\nCreate a function that uses the model's coefficient and intercept values to return a new value. This new value represents probability that the given observation is a tumor:\ndef logit2prob(logr,x):\nlog_odds = logr.coef_ * x + logr.intercept_\nodds = numpy.exp(log_odds)\nprobability = odds / (1 + odds)\nreturn(probability)\nFunction Explained\nTo find the log-odds for each observation, we must first create a formula that looks similar to the one from linear regression, extracting the coefficient and the intercept.\nlog_odds = logr.coef_ * x + logr.intercept_\nTo then convert the log-odds to odds we must exponentiate the log-odds.\nodds = numpy.exp(log_odds)\nNow that we have the odds, we can convert it to probability by dividing it by 1 plus the odds.\nprobability = odds / (1 + odds)\nLet us now use the function with what we have learned to find out the probability that each tumor is cancerous.\nExample\nSee the whole example in action:\nimport numpy\nfrom sklearn import linear_model\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\ndef logit2prob(logr, X):\nlog_odds = logr.coef_ * X + logr.intercept_\nodds = numpy.exp(log_odds)\nprobability = odds / (1 + odds)\nreturn(probability)\nprint(logit2prob(logr, X))\nResult\n[[0.60749955]\n   [0.19268876]\n   [0.12775886]\n   [0.00955221]\n   [0.08038616]\n   [0.07345637]\n   [0.88362743]\n   [0.77901378]\n   [0.88924409]\n   [0.81293497]\n   [0.57719129]\n   [0.96664243]]\nRun example \u00bb\nResults Explained\n3.78 0.61 The probability that a tumor with the size 3.78cm is cancerous is 61%.\n2.44 0.19 The probability that a tumor with the size 2.44cm is cancerous is 19%.\n2.09 0.13 The probability that a tumor with the size 2.09cm is cancerous is 13%.\n\n\u2605\n+1",
      "order": 2,
      "code": null,
      "language": null,
      "explanation": null
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 3,
      "code": "import numpy\nfrom sklearn import linear_model\n#Reshaped for Logistic function.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\n#predict if tumor is cancerous where the size is 3.46mm:\npredicted = logr.predict(numpy.array([3.46]).reshape(-1,1))\nprint(predicted)",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 4,
      "code": "import numpy\nfrom sklearn import linear_model\n#Reshaped for Logistic function.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\nlog_odds = logr.coef_\nodds = numpy.exp(log_odds)\nprint(odds)",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 5,
      "code": "import numpy\nfrom sklearn import linear_model\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\ndef logit2prob(logr, X):\nlog_odds = logr.coef_ * X + logr.intercept_\nodds = numpy.exp(log_odds)\nprobability = odds / (1 + odds)\nreturn(probability)\nprint(logit2prob(logr, X))",
      "language": "python",
      "explanation": "Example of example"
    }
  ],
  "practice_exercises": [
    {
      "title": "Complete the Code 1",
      "description": "Fill in the missing line to make this code work.",
      "difficulty": "medium",
      "starter_code": "import numpy\nfrom sklearn import linear_model\n#Reshaped for Logistic function.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n# TODO: Complete this line\nlogr.fit(X,y)\n#predict if tumor is cancerous where the size is 3.46mm:\npredicted = logr.predict(numpy.array([3.46]).reshape(-1,1))\nprint(predicted)",
      "solution": "import numpy\nfrom sklearn import linear_model\n#Reshaped for Logistic function.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\n#predict if tumor is cancerous where the size is 3.46mm:\npredicted = logr.predict(numpy.array([3.46]).reshape(-1,1))\nprint(predicted)"
    },
    {
      "title": "Complete the Code 2",
      "description": "Fill in the missing line to make this code work.",
      "difficulty": "medium",
      "starter_code": "import numpy\nfrom sklearn import linear_model\n#Reshaped for Logistic function.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\n# TODO: Complete this line\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\nlog_odds = logr.coef_\nodds = numpy.exp(log_odds)\nprint(odds)",
      "solution": "import numpy\nfrom sklearn import linear_model\n#Reshaped for Logistic function.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\nlog_odds = logr.coef_\nodds = numpy.exp(log_odds)\nprint(odds)"
    }
  ],
  "related_topics": [
    {
      "id": "fefff309-2e79-4950-9ef6-1b81e750de25",
      "title": "C Best Practices",
      "relationship": "related_topic"
    },
    {
      "id": "90f07a36-e34b-486f-be79-4cddf48a1a4a",
      "title": "C Common Pitfalls and How to Avoid Them",
      "relationship": "suggested_reading"
    },
    {
      "id": "f3585c4b-505e-4bc6-ab61-1e78d81ae935",
      "title": "C Fundamentals",
      "relationship": "prerequisite"
    }
  ],
  "quiz": [
    {
      "question": "What is In the simplest case there?",
      "options": [
        "called multinomial",
        "None of the above.",
        "None of the above.",
        "two outcomes"
      ],
      "correct_answer": 3,
      "explanation": "The correct definition of In the simplest case there is 'two outcomes'."
    },
    {
      "question": "What is in this case it?",
      "options": [
        "two outcomes",
        "called multinomial",
        "None of the above.",
        "None of the above."
      ],
      "correct_answer": 1,
      "explanation": "The correct definition of in this case it is 'called multinomial'."
    }
  ],
  "summary": "This tutorial covers Machine Learning - Logistic Regression concepts and techniques. You'll learn how to use Machine Learning - Logistic Regression effectively, including key principles, common patterns, and practical examples. By the end of this tutorial, you'll have a solid understanding of Machine Learning - Logistic Regression and how to apply it in your projects."
}