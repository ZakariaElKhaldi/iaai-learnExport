{
  "id": "dc6090ec-7d66-4db6-824d-9231b2604a2e",
  "title": "Machine Learning - Confusion Matrix",
  "slug": "machine-learning-confusion-matrix",
  "metadata": {
    "description": "Learn about Machine Learning - Confusion Matrix with clear explanations and practical examples.",
    "keywords": [
      "c",
      "predicted",
      "metrics",
      "example",
      "positive",
      "true",
      "actual",
      "negative",
      "confusion",
      "matrix",
      "false"
    ],
    "difficulty": "intermediate",
    "prerequisites": [
      "C Basics"
    ],
    "estimated_time": 5,
    "category": "LearnPython",
    "subcategory": "Confusion Matrix"
  },
  "content_sections": [
    {
      "type": "introduction",
      "title": "Introduction",
      "content": "Machine Learning - Confusion Matrix",
      "order": 1,
      "code": null,
      "language": null,
      "explanation": null
    },
    {
      "type": "concept",
      "title": "Details",
      "content": "On this page, .com collaborates with\nNYC Data Science Academy\n, to deliver digital training content to our students.\nWhat is a confusion matrix?\nIt is a table that is used in classification problems to assess where errors in the model were made.\nThe rows represent the actual classes the outcomes should have been.\nWhile the columns represent the predictions we have made.\nUsing this table it is easy to see which predictions are wrong.\nCreating a Confusion Matrix\nConfusion matrixes can be created by predictions made from a logistic regression.\nFor now we will generate actual and predicted values by utilizing NumPy:\nimport numpy\n\nactual = numpy.random.binomial(1, 0.9, size = 1000)\npredicted = numpy.random.binomial(1, 0.9, size = 1000)\nIn order to create the confusion matrix we need to import metrics from the sklearn module.\nfrom sklearn import metrics\nOnce metrics is imported we can use the confusion matrix function on our actual and predicted values.\nconfusion_matrix = metrics.confusion_matrix(actual, predicted)\nTo create a more interpretable visual display we need to convert the table into a confusion matrix display.\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, \n1])\nVizualizing the display requires that we import pyplot from matplotlib.\nimport matplotlib.pyplot as plt\nFinally to display the plot we can use the functions plot() and show() from pyplot.\ncm_display.plot()\nplt.show()\nSee the whole example in action:\nExample\nimport matplotlib.pyplot as plt\nimport numpy\nfrom sklearn import metrics\nactual = numpy.random.binomial(1,.9,size = 1000)\npredicted = \n  numpy.random.binomial(1,.9,size = 1000)\nconfusion_matrix = \n  metrics.confusion_matrix(actual, predicted)\ncm_display = \n  metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, \n  display_labels = [0, 1])\ncm_display.plot()\nplt.show()\nResult\nRun example \u00bb\nResults Explained\nThe Confusion Matrix created has four different quadrants:\nTrue Negative (Top-Left Quadrant)\nFalse Positive (Top-Right Quadrant)\nFalse Negative (Bottom-Left Quadrant)\nTrue Positive (Bottom-Right Quadrant)\nTrue means that the values were accurately predicted, False means that there was an error or wrong prediction.\nNow that we have made a Confusion Matrix, we can calculate different measures to quantify the quality of the model. First, lets look at Accuracy.\nADVERTISEMENT\nCreated Metrics\nThe matrix provides us with many useful metrics that help us to evaluate our classification model.\nThe different measures include: Accuracy, Precision, Sensitivity (Recall), Specificity, and the F-score, explained below.\nAccuracy\nAccuracy measures how often the model is correct.\nHow to Calculate\n(True Positive + True Negative) / Total Predictions\nExample\nAccuracy = metrics.accuracy_score(actual, predicted)\nRun example \u00bb\nPrecision\nOf the positives predicted, what percentage is truly positive?\nHow to Calculate\nTrue Positive / (True Positive + False Positive)\nPrecision does not evaluate the correctly predicted negative cases:\nExample\nPrecision = metrics.precision_score(actual, predicted)\nRun example \u00bb\nSensitivity (Recall)\nOf all the positive cases, what percentage are predicted positive?\nSensitivity (sometimes called Recall) measures how good the model is at predicting positives.\nThis means it looks at true positives and false negatives (which are positives that have been incorrectly predicted as negative).\nHow to Calculate\nTrue Positive / (True Positive + False Negative)\nSensitivity is good at understanding how well the model predicts something is positive:\nExample\nSensitivity_recall = metrics.recall_score(actual, predicted)\nRun example \u00bb\nSpecificity\nHow well the model is at prediciting negative results?\nSpecificity is similar to sensitivity, but looks at it from the persepctive of negative results.\nHow to Calculate\nTrue Negative / (True Negative + False Positive)\nSince it is just the opposite of Recall, we use the recall_score function, taking the opposite position label:\nExample\nSpecificity = metrics.recall_score(actual, predicted, pos_label=0)\nRun example \u00bb\nF-score\nF-score is the \"harmonic mean\" of precision and sensitivity.\nIt considers both false positive and false negative cases and is good for imbalanced datasets.\nHow to Calculate\n2 * ((Precision * Sensitivity) / (Precision + Sensitivity))\nThis score does not take into consideration the True Negative values:\nExample\nF1_score = metrics.f1_score(actual, predicted)\nRun example \u00bb\nAll calulations in one:\nExample\n#metrics\nprint({\"Accuracy\":Accuracy,\"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score})\nRun example \u00bb\n\n\u2605\n+1",
      "order": 2,
      "code": null,
      "language": null,
      "explanation": null
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 3,
      "code": "import matplotlib.pyplot as plt\nimport numpy\nfrom sklearn import metrics\nactual = numpy.random.binomial(1,.9,size = 1000)\npredicted = \n  numpy.random.binomial(1,.9,size = 1000)\nconfusion_matrix = \n  metrics.confusion_matrix(actual, predicted)\ncm_display = \n  metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, \n  display_labels = [0, 1])\ncm_display.plot()\nplt.show()",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 4,
      "code": "Accuracy = metrics.accuracy_score(actual, predicted)",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 5,
      "code": "Precision = metrics.precision_score(actual, predicted)",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 6,
      "code": "Sensitivity_recall = metrics.recall_score(actual, predicted)",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 7,
      "code": "Specificity = metrics.recall_score(actual, predicted, pos_label=0)",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 8,
      "code": "F1_score = metrics.f1_score(actual, predicted)",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 9,
      "code": "#metrics\nprint({\"Accuracy\":Accuracy,\"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score})",
      "language": "python",
      "explanation": "Example of example"
    }
  ],
  "practice_exercises": [
    {
      "title": "Complete the Code 1",
      "description": "Fill in the missing line to make this code work.",
      "difficulty": "medium",
      "starter_code": "import matplotlib.pyplot as plt\nimport numpy\n# TODO: Complete this line\nactual = numpy.random.binomial(1,.9,size = 1000)\npredicted = \n  numpy.random.binomial(1,.9,size = 1000)\nconfusion_matrix = \n  metrics.confusion_matrix(actual, predicted)\ncm_display = \n  metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, \n  display_labels = [0, 1])\ncm_display.plot()\nplt.show()",
      "solution": "import matplotlib.pyplot as plt\nimport numpy\nfrom sklearn import metrics\nactual = numpy.random.binomial(1,.9,size = 1000)\npredicted = \n  numpy.random.binomial(1,.9,size = 1000)\nconfusion_matrix = \n  metrics.confusion_matrix(actual, predicted)\ncm_display = \n  metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, \n  display_labels = [0, 1])\ncm_display.plot()\nplt.show()"
    },
    {
      "title": "Practice Exercise 2",
      "description": "Write code that implements similar functionality.",
      "difficulty": "easy",
      "starter_code": "# Write your python code here",
      "solution": "Accuracy = metrics.accuracy_score(actual, predicted)"
    }
  ],
  "related_topics": [
    {
      "id": "a1ed2e7c-4dae-498f-943f-2a48b5e1d391",
      "title": "C Best Practices",
      "relationship": "related_topic"
    },
    {
      "id": "10b87bed-1558-4337-bf19-f5a71a988603",
      "title": "C Common Pitfalls and How to Avoid Them",
      "relationship": "suggested_reading"
    },
    {
      "id": "e17444dc-c15c-4966-a95a-b10cdb929080",
      "title": "C Fundamentals",
      "relationship": "prerequisite"
    }
  ],
  "quiz": [
    {
      "question": "What is What?",
      "options": [
        "None of the above.",
        "used in classification problems to assess where errors in the model were made",
        "None of the above.",
        "a confusion matrix"
      ],
      "correct_answer": 3,
      "explanation": "The correct definition of What is 'a confusion matrix'."
    },
    {
      "question": "What is It is a table that?",
      "options": [
        "None of the above.",
        "a confusion matrix",
        "None of the above.",
        "used in classification problems to assess where errors in the model were made"
      ],
      "correct_answer": 3,
      "explanation": "The correct definition of It is a table that is 'used in classification problems to assess where errors in the model were made'."
    }
  ],
  "summary": "This tutorial covers Machine Learning - Confusion Matrix concepts and techniques. You'll learn how to use Machine Learning - Confusion Matrix effectively, including key principles, common patterns, and practical examples. By the end of this tutorial, you'll have a solid understanding of Machine Learning - Confusion Matrix and how to apply it in your projects."
}