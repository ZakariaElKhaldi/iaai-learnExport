{
  "id": "86cd7eeb-a04f-407d-94a5-8658d025e278",
  "title": "Machine Learning - Cross Validation",
  "slug": "machine-learning-cross-validation",
  "metadata": {
    "description": "Learn about Machine Learning - Cross Validation with clear explanations and practical examples.",
    "keywords": [
      "c",
      "scores",
      "import",
      "sklearn",
      "validation",
      "print",
      "cross",
      "decisiontreeclassifier",
      "average",
      "datasets",
      "number"
    ],
    "difficulty": "intermediate",
    "prerequisites": [
      "C Basics"
    ],
    "estimated_time": 5,
    "category": "LearnPython",
    "subcategory": "Cross Validation"
  },
  "content_sections": [
    {
      "type": "introduction",
      "title": "Introduction",
      "content": "Machine Learning - Cross Validation",
      "order": 1,
      "code": null,
      "language": null,
      "explanation": null
    },
    {
      "type": "concept",
      "title": "Details",
      "content": "On this page, .com collaborates with\nNYC Data Science Academy\n, to deliver digital training content to our students.\nCross Validation\nWhen adjusting models we are aiming to increase overall model performance on unseen data. Hyperparameter tuning can lead to much better performance on test sets. However, optimizing parameters to the test set can lead information leakage causing the model to preform worse on unseen data. To correct for this we can perform cross validation.\nTo better understand CV, we will be performing different methods on the iris dataset. Let us first load in and separate the data.\nfrom sklearn import datasets\nX, y = datasets.load_iris(return_X_y=True)\nThere are many methods to cross validation, we will start by looking at k-fold cross validation.\nK\n-Fold\nThe training data used in the model is split, into k number of smaller sets, to be used to validate the model. The model is then trained on k-1 folds of training set. The remaining fold is then used as a validation set to evaluate the model.\nAs we will be trying to classify different species of iris flowers we will need to import a classifier model, for this exercise we will be using a\nDecisionTreeClassifier\n. We will also need to import CV modules from\nsklearn\n.\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nWith the data loaded we can now create and fit a model for evaluation.\nclf = DecisionTreeClassifier(random_state=42)\nNow let's evaluate our model and see how it performs on each\nk\n-fold.\nk_folds = KFold(n_splits = 5)\nscores = cross_val_score(clf, X, y, cv = k_folds)\nIt is also good pratice to see how CV performed overall by averaging the scores for all folds.\nExample\nRun k-fold CV:\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nk_folds = KFold(n_splits = 5)\nscores = cross_val_score(clf, X, y, cv = k_folds)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))\nRun example \u00bb\nADVERTISEMENT\nStratified K-Fold\nIn cases where classes are imbalanced we need a way to account for the imbalance in both the train and validation sets. To do so we can stratify the target classes, meaning that both sets will have an equal proportion of all classes.\nExample\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nsk_folds = StratifiedKFold(n_splits = 5)\nscores = cross_val_score(clf, X, y, cv = sk_folds)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))\nRun example \u00bb\nWhile the number of folds is the same, the average CV increases from the basic k-fold when making sure there is stratified classes.\nLeave-One-Out (LOO)\nInstead of selecting the number of splits in the training data set like k-fold LeaveOneOut, utilize 1 observation to validate and n-1 observations to train. This method is an exaustive technique.\nExample\nRun LOO CV:\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import LeaveOneOut, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nloo = LeaveOneOut()\nscores = cross_val_score(clf, X, y, cv = loo)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))\nRun example \u00bb\nWe can observe that the number of cross validation scores performed is equal to the number of observations in the dataset. In this case there are 150 observations in the iris dataset.\nThe average CV score is 94%.\nLeave-P-Out (LPO)\nLeave-P-Out is simply a nuanced diffence to the Leave-One-Out idea, in that we can select the number of p to use in our validation set.\nExample\nRun LPO CV:\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import LeavePOut, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nlpo = LeavePOut(p=2)\nscores = cross_val_score(clf, X, y, cv = lpo)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))\nRun example \u00bb\nAs we can see this is an exhaustive method we many more scores being calculated than Leave-One-Out, even with a p = 2, yet it achieves roughly the same average CV score.\nShuffle Split\nUnlike\nKFold\n,\nShuffleSplit\nleaves out a percentage of the data, not to be used in the train or validation sets. To do so we must decide what the train and test sizes are, as well as the number of splits.\nExample\nRun Shuffle Split CV:\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import ShuffleSplit, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nss = ShuffleSplit(train_size=0.6, test_size=0.3, n_splits = 5)\nscores = cross_val_score(clf, X, y, cv = ss)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))\nRun example \u00bb\nEnding Notes\nThese are just a few of the CV methods that can be applied to models. There are many more cross validation classes, with most models having their own class. Check out sklearns cross validation for more CV options.\n\n\u2605\n+1",
      "order": 2,
      "code": null,
      "language": null,
      "explanation": null
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 3,
      "code": "from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nk_folds = KFold(n_splits = 5)\nscores = cross_val_score(clf, X, y, cv = k_folds)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 4,
      "code": "from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nsk_folds = StratifiedKFold(n_splits = 5)\nscores = cross_val_score(clf, X, y, cv = sk_folds)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 5,
      "code": "from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import LeaveOneOut, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nloo = LeaveOneOut()\nscores = cross_val_score(clf, X, y, cv = loo)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 6,
      "code": "from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import LeavePOut, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nlpo = LeavePOut(p=2)\nscores = cross_val_score(clf, X, y, cv = lpo)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))",
      "language": "python",
      "explanation": "Example of example"
    },
    {
      "type": "code_example",
      "title": "Example",
      "content": "",
      "order": 7,
      "code": "from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import ShuffleSplit, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nss = ShuffleSplit(train_size=0.6, test_size=0.3, n_splits = 5)\nscores = cross_val_score(clf, X, y, cv = ss)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))",
      "language": "python",
      "explanation": "Example of example"
    }
  ],
  "practice_exercises": [
    {
      "title": "Complete the Code 1",
      "description": "Fill in the missing line to make this code work.",
      "difficulty": "medium",
      "starter_code": "from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\n# TODO: Complete this line\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nk_folds = KFold(n_splits = 5)\nscores = cross_val_score(clf, X, y, cv = k_folds)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))",
      "solution": "from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nk_folds = KFold(n_splits = 5)\nscores = cross_val_score(clf, X, y, cv = k_folds)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))"
    },
    {
      "title": "Complete the Code 2",
      "description": "Fill in the missing line to make this code work.",
      "difficulty": "medium",
      "starter_code": "from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\n# TODO: Complete this line\nsk_folds = StratifiedKFold(n_splits = 5)\nscores = cross_val_score(clf, X, y, cv = sk_folds)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))",
      "solution": "from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(random_state=42)\nsk_folds = StratifiedKFold(n_splits = 5)\nscores = cross_val_score(clf, X, y, cv = sk_folds)\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))"
    }
  ],
  "related_topics": [
    {
      "id": "ae92c138-0bbd-42b9-a14e-0eadbf7fa3d0",
      "title": "C Best Practices",
      "relationship": "related_topic"
    },
    {
      "id": "60feb67d-8059-45e3-98fb-7ba17e354f43",
      "title": "C Common Pitfalls and How to Avoid Them",
      "relationship": "suggested_reading"
    },
    {
      "id": "956b2497-e5b9-4463-9abd-9e08ce14f094",
      "title": "C Fundamentals",
      "relationship": "prerequisite"
    }
  ],
  "quiz": [
    {
      "question": "What is Cross Validation\nWhen adjusting models we?",
      "options": [
        "None of the above.",
        "aiming to increase overall model performance on unseen data",
        "many methods to cross validation",
        "None of the above."
      ],
      "correct_answer": 1,
      "explanation": "The correct definition of Cross Validation\nWhen adjusting models we is 'aiming to increase overall model performance on unseen data'."
    },
    {
      "question": "What is There?",
      "options": [
        "aiming to increase overall model performance on unseen data",
        "None of the above.",
        "many methods to cross validation",
        "None of the above."
      ],
      "correct_answer": 2,
      "explanation": "The correct definition of There is 'many methods to cross validation'."
    }
  ],
  "summary": "This tutorial covers Machine Learning - Cross Validation concepts and techniques. You'll learn how to use Machine Learning - Cross Validation effectively, including key principles, common patterns, and practical examples. By the end of this tutorial, you'll have a solid understanding of Machine Learning - Cross Validation and how to apply it in your projects."
}